{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from data_prep_scripts.data_countries import get_country_of_origin_data \n",
    "from data_prep_scripts.data_manipulation import df_replaceColVals_vars,process_remaining_categ_cols\n",
    "from data_prep_scripts.process_repetitive_cols import get_repetitive_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column we predict\n",
    "col_to_predict = 'Triple Negative'\n",
    "# preparing the df containing the raw data\n",
    "medData = pd.read_excel('Merged File 2.5.19 De-identified.xlsx')\n",
    "medData['Country of Origin for Father'].replace(76,'Ireland',inplace=True)\n",
    "medData = medData[(medData['Breast Cancer?']=='Yes') & (medData['Gender'] == 'Female')]\n",
    "medData = medData.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "predCol = pd.read_excel('Additional BCD Info 3.1.19 De-identified.xlsx',usecols=\"A,DZ\")\n",
    "medData = medData.merge(predCol,left_on='ID #', right_on='ID #')\n",
    "medData = medData[medData[col_to_predict]!='Unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_repititive_blocks = [\n",
    "    (47,89,7),(89,117,7),(117,152,7),\n",
    "    (163,191,7),(197,213,4),(214,249,5),\n",
    "    (294,348,6),(349,356,7),(357,364,7),\n",
    "    (427,435,8)\n",
    "]\n",
    "ind_not_del = list(range(22,43)) + [274]\n",
    "ind_to_del = [2,3,4,(8,11),13,451,(197,213),(349,356),(386,414),(448,469)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processes blocks of columns that contain the same type of information\n",
    "# groups them and returns them in lists.\n",
    "rep_cols = get_repetitive_cols(medData)\n",
    "# remove empty blocks and replace block[1] with an empty list\n",
    "# if there is no cont. element\n",
    "rep_cols_np = [\n",
    "    [\n",
    "        (block[0].astype('int64').values, block[1].astype('float32').values)\n",
    "        if isinstance(block[1],pd.DataFrame) and block[1].shape[1]>0 \n",
    "        else (block[0].astype('int64').values,[]) for block in group  if block\n",
    "    ] \n",
    "    for group in rep_cols\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features for country of origin columns\n",
    "country_of_origin_data = get_country_of_origin_data(medData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixed_cols=[]\n",
    "# for ii, (a,b) in enumerate(medData.dtypes.iteritems()):\n",
    "#     if b == object:\n",
    "#         types = set([type(k) for k in list(medData[a].dropna() ) ])\n",
    "#         if str in types and len(types)>1: \n",
    "#             print(ii,a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns that should only contain real numbered values, but contain strings in the raw data.\n",
    "# this part of the code saves such string values in a json file, where we can specify\n",
    "# the values to replace them with.\n",
    "mixedCols_write_strs = [92,121,126,131,136,141,146,151,152,192,294,303,416,417,418,446,449]\n",
    "dict_replaceColVals = df_replaceColVals_vars(medData,mixedCols_write_strs,str_vals=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "medData.replace(dict_replaceColVals,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/serkan/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/serkan/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/serkan/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:140: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/home/serkan/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:110: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/serkan/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:132: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# replace outlier values with the values given in the json file\n",
    "# where we can specify the values to replace them with\n",
    "dict_replaceColVals_cont = df_replaceColVals_vars(medData,str_vals=False, cont_vals=True)\n",
    "for k,v in dict_replaceColVals_cont.items():\n",
    "    k_l, v_l = [], []\n",
    "    for key, val in v.items():\n",
    "        k_l.append(float(key)); v_l.append(val)\n",
    "    medData[k].replace(k_l,v_l,inplace=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_min_max_count = 250\n",
    "val_count_threshold = 50\n",
    "col_values_replace_OTH = defaultdict(lambda:[])\n",
    "cols_to_del=[]\n",
    "for c in ind_to_del: \n",
    "    if isinstance(c,tuple): cols_to_del += list(medData.columns[c[0]:c[1]])\n",
    "    else: cols_to_del.append(medData.columns[c])\n",
    "\n",
    "for ii, (a,b) in enumerate(medData.dtypes.iteritems()):\n",
    "    # do not delete the column in repetitive blocks\n",
    "    in_rep_block = False\n",
    "    for block in ind_repititive_blocks: \n",
    "        if (ii>=block[0]) and (ii < block[1]): in_rep_block = True\n",
    "            \n",
    "    if (not in_rep_block) and (b == object) and (ii not in ind_not_del):\n",
    "        col_counts = medData[a].value_counts()        \n",
    "        col_max_count = col_counts.iloc[0]\n",
    "        # the most frequent value of a column should occur at least 'col_min_max_count' times\n",
    "        if col_max_count < col_min_max_count: cols_to_del.append(a)\n",
    "        elif medData.shape[0]-col_max_count < col_min_max_count: cols_to_del.append(a)\n",
    "#         # for the columns that we preserve, we replace values if their \n",
    "#         # frequency is not above the given threshold 'val_count_threshold'.\n",
    "#         else:\n",
    "#             vals_to_OTH = list(col_counts[col_counts<val_count_threshold].index)\n",
    "#             if vals_to_OTH: \n",
    "#                 col_values_replace_OTH[a] = vals_to_OTH\n",
    "# print(len(col_values_replace_OTH.keys()))\n",
    "# print(col_values_replace_OTH.keys())\n",
    "# print(len(cols_to_del))\n",
    "# print(cols_to_del)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I deleted the datetime values for now. Will be added in the next iterations.\n",
    "cols_to_del += list(medData.select_dtypes(include=['datetime']))\n",
    "# cols processed by the 'get_repetitive_cols()' function\n",
    "cols_repetitive = sum([list(medData.columns[c[0]:c[1]]) for c in ind_repititive_blocks],[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_obj_cols = [\n",
    "    k for k in medData.select_dtypes(include=['object']) \n",
    "        if (k not in cols_to_del + cols_repetitive +[col_to_predict]) and \n",
    "           (not medData[k].isnull().all() )    \n",
    "]\n",
    "remaining_float_cols = [\n",
    "    k for k in medData.select_dtypes(include=['float64']) \n",
    "        if (k not in cols_to_del + cols_repetitive +[col_to_predict]) and \n",
    "           (not medData[k].isnull().all() )    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "medData_categ = process_remaining_categ_cols(medData[remaining_obj_cols])\n",
    "medData_float = medData[remaining_float_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rep_cols\n",
    "#country_of_origin_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i,k in enumerate(medData.columns):\n",
    "#     if k in remaining_obj_cols: print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model's inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation splits (80% and 20% respectively)\n",
    "n_patients = medData.shape[0]\n",
    "temp_indices = np.arange(n_patients)\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(temp_indices)\n",
    "train_ind, valid_ind = [\n",
    "    temp_indices[ int(n_patients*c[0]):int(n_patients*c[1]) ]\n",
    "    for c in [(0,0.8), (.8,1.)]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single, unique columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# floats\n",
    "float_fields_scaler = StandardScaler()\n",
    "float_fields_scaler.fit(medData_float.iloc[train_ind])\n",
    "\n",
    "train_x_f = np.nan_to_num(\n",
    "    float_fields_scaler.transform(medData_float.iloc[train_ind])\n",
    ").astype('float32')\n",
    "valid_x_f = np.nan_to_num(\n",
    "    float_fields_scaler.transform(medData_float.iloc[valid_ind])\n",
    ").astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical variables\n",
    "train_x_c = medData_categ.iloc[train_ind].values.astype('int64')\n",
    "valid_x_c = medData_categ.iloc[valid_ind].values.astype('int64')\n",
    "categ_vars_max_vals = list(medData_categ.values.max(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blocks of columns that repeat several times (e.g. `1. relative with condition x`, `1. relative age`, `2. relative with condition x`, `2. relative age` etc.). They are processed sepearately in a way that would allow parameter sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_rep_list_np = [] \n",
    "valid_x_rep_list_np = [] \n",
    "std_scalers_f_rep = []\n",
    "categ_data_rep_maxvals = [] # redundent but useful for now\n",
    "rep_blocks_dims_list = [] #list of tuples of (n_rep, c_n_fields, f_n_fields, c_max_vals)\n",
    "\n",
    "x_rep_list = []\n",
    "for group in rep_cols_np:\n",
    "    rep_cols_c, rep_cols_f = list(zip(*group))\n",
    "    rep_cols_c, rep_cols_f = np.array(rep_cols_c), np.array(rep_cols_f)\n",
    "    # categorical values\n",
    "    g_n_rep, g_n_batch, g_n_fields_c = rep_cols_c.shape\n",
    "    rep_cols_c = rep_cols_c.transpose((1,0,2)).reshape(-1,g_n_fields_c)\n",
    "    categ_data_rep_maxvals.append(list(rep_cols_c.max(axis=0)))\n",
    "    rep_cols_c = rep_cols_c.reshape(g_n_batch,g_n_rep,g_n_fields_c)\n",
    "    # cont values\n",
    "    if rep_cols_f.ndim == 3:\n",
    "        g_n_fields_f = rep_cols_f.shape[-1]\n",
    "        rep_blocks_dims_list.append((g_n_rep,g_n_fields_c,g_n_fields_f, categ_data_rep_maxvals[-1]))\n",
    "        rep_cols_f = rep_cols_f.transpose((1,0,2)).reshape(-1,g_n_fields_f)\n",
    "        g_scaler = StandardScaler()\n",
    "        g_scaler.fit(rep_cols_f[train_ind])\n",
    "        std_scalers_f_rep.append(g_scaler)\n",
    "        rep_cols_f = np.nan_to_num(g_scaler.transform(rep_cols_f))\n",
    "        rep_cols_f = rep_cols_f.reshape(g_n_batch,1,g_n_rep,g_n_fields_f)\n",
    "    else: \n",
    "        rep_blocks_dims_list.append((g_n_rep,g_n_fields_c,0, categ_data_rep_maxvals[-1]))\n",
    "        std_scalers_f_rep.append(None)\n",
    "        rep_cols_f = None\n",
    "    \n",
    "    train_x_rep_list_np.append((\n",
    "        rep_cols_c[train_ind],\n",
    "        rep_cols_f[train_ind] if isinstance(rep_cols_f,np.ndarray) else None \n",
    "        ))\n",
    "    valid_x_rep_list_np.append((\n",
    "        rep_cols_c[valid_ind],\n",
    "        rep_cols_f[valid_ind] if isinstance(rep_cols_f,np.ndarray) else None \n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predicted variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = (medData[col_to_predict] == 'Yes').values.astype('int64')\n",
    "y_train, y_valid = y[train_ind], y[valid_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the dataset and prepare the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NYULH_DS(Dataset):\n",
    "    def __init__(self, x_f, x_c, x_rep_list_np, y):\n",
    "        super(NYULH_DS, self).__init__()\n",
    "        self.batch_size = x_f.shape[0]\n",
    "        self.x_rep_list_np = x_rep_list_np\n",
    "        assert x_f.shape[0] == x_c.shape[0], 'number of rows do not match'\n",
    "        for ii, arr in enumerate(x_rep_list_np): \n",
    "            assert x_f.shape[0] == arr[0].shape[0],\\\n",
    "            f'number of rows do not match for the categorical data at index {ii}'\n",
    "            if isinstance(arr[1],np.ndarray) : \n",
    "                assert x_f.shape[0] == arr[1].shape[0], \\\n",
    "                f'number of rows do not match for the cont. data at index {ii}'\n",
    "        \n",
    "        self.x_f = x_f\n",
    "        self.x_c = x_c\n",
    "        self.x_f_rep_list_np = x_rep_list_np\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return self.batch_size\n",
    "        \n",
    "    def __getitem__(self, i):     \n",
    "        return (\n",
    "            (\n",
    "                self.x_f[i], \n",
    "                self.x_c[i], \n",
    "                [(k[0][i],k[1][i] if isinstance(k[1],np.ndarray) else np.array(()))\n",
    "                 for k in self.x_rep_list_np]\n",
    "            ),\n",
    "            y[i]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = NYULH_DS(train_x_f, train_x_c, train_x_rep_list_np, y[train_ind])\n",
    "ds_valid = NYULH_DS(valid_x_f, valid_x_c, valid_x_rep_list_np, y[valid_ind])\n",
    "train_loader = DataLoader(ds_train,batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(ds_valid,batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_On_Blocks(nn.Module):\n",
    "    '''\n",
    "    This module is used for inputs that consists blocks of repetitive information, for example:\n",
    "    `1. relative with condition x`, `1. relative age`, `2. relative with condition x`, `2. relative age`,..\n",
    "    In such cases, we should be indifferent to where the information is given, and use it the same way\n",
    "    regardless of the block that it is given in. We induce this behavior by using convolution operations\n",
    "    and by doing parameter sharing.\n",
    "    Input of the model is a tuple of (`categorical vars`, `continuous vars`)\n",
    "    '''\n",
    "    def __init__(\n",
    "        self, n_rep, c_n_fields, f_n_fields, c_max_vals, n_hid_func=None, n_emb_func=None\n",
    "    ):\n",
    "        super(Conv_On_Blocks, self).__init__()\n",
    "        assert n_rep>=0 and c_n_fields >= 0 and f_n_fields>=0 and c_n_fields+f_n_fields> 0, 'conv_on_block: invalid input'\n",
    "        self.n_rep = n_rep\n",
    "        self.c_n_fields = c_n_fields\n",
    "        self.f_n_fields = f_n_fields\n",
    "        self.categ_input, self.float_input = c_n_fields>0, f_n_fields>0\n",
    "        \n",
    "        self.n_emb_func = n_emb_func if n_emb_func else lambda x: int(np.sqrt(x))+1 \n",
    "        self.n_hid_func = n_hid_func if n_hid_func else lambda x: int(np.round(x/2+1))\n",
    "        \n",
    "        if self.categ_input:\n",
    "            self.embeddings = nn.ModuleList(\n",
    "                [nn.Embedding(k+1,self.n_emb_func(k+1)) for k in c_max_vals]\n",
    "            )\n",
    "            self.c_n_hidden = sum([self.n_emb_func(k+1) for k in c_max_vals])\n",
    "        else:\n",
    "            self.c_n_hidden = 0\n",
    "            \n",
    "        self.n_hidden_1 = max(3, self.n_hid_func(self.f_n_fields+self.c_n_hidden))\n",
    "        self.n_hidden_2 = max(3, self.n_hid_func(self.n_hidden_1))\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=self.n_hidden_1, kernel_size=(1,self.f_n_fields+self.c_n_hidden))\n",
    "        self.conv2 = nn.Conv2d(in_channels=1, out_channels=self.n_hidden_2, kernel_size=(1,self.n_hidden_1))\n",
    "        self.fc1 = nn.Linear(self.n_hidden_2, self.n_hidden_2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.categ_input:\n",
    "            x_c = x[0].view(-1,self.c_n_fields)            \n",
    "            x_c = torch.cat(\n",
    "                [self.embeddings[k](x_c[:,k]) for k in range(self.c_n_fields)],\n",
    "                dim=1\n",
    "            ).reshape(-1, 1, self.n_rep, self.c_n_hidden)\n",
    "            \n",
    "            if self.float_input: x = torch.cat([x_c,x[1]],dim=-1)\n",
    "            else: x = x_c\n",
    "        else:\n",
    "            x = x[1]\n",
    "            \n",
    "        x = F.leaky_relu(self.conv1(x), negative_slope=0.2, inplace=True)\n",
    "        x = x.transpose(1,3)\n",
    "        x = F.leaky_relu(self.conv2(x), 0.2, True).view(-1, self.n_hidden_2, self.n_rep)\n",
    "        x = F.adaptive_avg_pool2d(x,(self.n_hidden_2,1))\n",
    "        x = x.view(-1, self.n_hidden_2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(\n",
    "        self, n_float_fields, n_categ_fields,categ_vars_max_vals, rep_blocks_dims_list, latent_size,\n",
    "        x_drop_p = 0.2, h_drop_p=0.5, z_noise=1., train_mode=True, n_emb_func=None, autoencoder=False\n",
    "    ):\n",
    "        super(Net, self).__init__()\n",
    "        self.n_float_fields = n_float_fields\n",
    "        self.n_categ_fields = n_categ_fields\n",
    "        self.categ_vars_max_vals = categ_vars_max_vals\n",
    "        self.rep_blocks_dims_list = rep_blocks_dims_list\n",
    "        self.latent_size = latent_size\n",
    "        self.x_drop_p = x_drop_p\n",
    "        self.h_drop_p = h_drop_p\n",
    "        self.z_noise = z_noise\n",
    "        self.train_mode = train_mode\n",
    "        self.autoencoder = autoencoder\n",
    "        \n",
    "        self.n_emb_func = n_emb_func if n_emb_func else lambda x: int(np.sqrt(x))+1 \n",
    "        \n",
    "        self.x_embeddings = nn.ModuleList(\n",
    "            [nn.Embedding(k+1,self.n_emb_func(k+1)) for k in categ_vars_max_vals]\n",
    "        )\n",
    "        self.c_n_hidden = sum([self.n_emb_func(k+1) for k in categ_vars_max_vals])\n",
    "        \n",
    "        self.x_rep_module = nn.ModuleList(\n",
    "            [Conv_On_Blocks(*k) for k in rep_blocks_dims_list]\n",
    "        )\n",
    "        self.rep_n_hidden = sum([k.fc1.out_features for k in self.x_rep_module])\n",
    "        \n",
    "        self.n_hidden_0 = self.c_n_hidden + self.rep_n_hidden + n_float_fields\n",
    "        self.n_hidden_1 = int(self.n_hidden_0/2 )+1\n",
    "        self.n_hidden_2 = int((self.n_hidden_1+latent_size)/2)\n",
    "        self.n_hidden_y0 = int(latent_size*3/5)\n",
    "        self.n_hidden_d1 = int((n_float_fields+n_categ_fields)*.7)\n",
    "        \n",
    "        self.enc_linear1 = nn.Linear(self.n_hidden_0,self.n_hidden_1)\n",
    "        self.enc_linear2 = nn.Linear(self.n_hidden_1,self.n_hidden_2)\n",
    "        self.enc_linear3 = nn.Linear(self.n_hidden_2,self.latent_size)\n",
    "        \n",
    "        \n",
    "        self.dec_linear1 = nn.Linear(self.latent_size,self.n_hidden_d1)\n",
    "        self.dec_linear2 = nn.Linear(self.n_hidden_d1,n_float_fields+n_categ_fields)\n",
    "        \n",
    "        self.y_linear1 = nn.Linear(self.latent_size,self.n_hidden_y0)\n",
    "        self.y_linear2 = nn.Linear(self.n_hidden_y0,1)\n",
    "                \n",
    "    def forward(self, x, verbose=False):\n",
    "        x = torch.cat(\n",
    "            [x[0]] +\\\n",
    "            [self.x_embeddings[k](x[1][:,k]) for k in range(self.n_categ_fields)] +\\\n",
    "            [x_rep_module(k) for k in x[2]],\n",
    "            dim = -1            \n",
    "        )\n",
    "        x = F.leaky_relu(self.enc_linear1(F.dropout(x,self.x_drop_p,self.train_mode,inplace=True)), .2,True)\n",
    "        x = F.leaky_relu(self.enc_linear2(F.dropout(x,self.h_drop_p,self.train_mode,inplace=True)), .2,True)\n",
    "        z = self.enc_linear3(F.dropout(x,self.h_drop_p,self.train_mode,inplace=True))\n",
    "        x = z + torch.randn_like(z, requires_grad=False)*self.z_noise if self.train_mode else z\n",
    "        \n",
    "        if autoencoder:            \n",
    "            x = F.leaky_relu(self.dec_linear1(x), .2,True)\n",
    "            x = self.dec_linear2(F.dropout(x,self.h_drop_p,self.train_mode,inplace=True))\n",
    "            return x,z\n",
    "        else:\n",
    "            x = F.leaky_relu(self.y_linear1(x), .2,True)\n",
    "            x = self.y_linear2(F.dropout(x,self.h_drop_p,self.train_mode,inplace=True))\n",
    "            return x,z\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x,y in train_loader:\n",
    "#     break\n",
    "# temp_b_conv = Conv_On_Blocks(7,5,3,categ_data_rep_maxvals[0])\n",
    "# temp_b_conv(x[2][0]).shape\n",
    "\n",
    "# tempnet = Net(16,81,categ_vars_max_vals,rep_blocks_dims_list,20)\n",
    "# tempnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
