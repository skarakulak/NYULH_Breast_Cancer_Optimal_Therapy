{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from data_prep_scripts.data_countries import get_country_of_origin_data \n",
    "from data_prep_scripts.data_manipulation import df_replaceColVals_vars,process_remaining_categ_cols\n",
    "from data_prep_scripts.process_repetitive_cols import get_repetitive_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column we predict\n",
    "col_to_predict = 'Triple Negative'\n",
    "# preparing the df containing the raw data\n",
    "medData = pd.read_excel('Merged File 2.5.19 De-identified.xlsx')\n",
    "medData['Country of Origin for Father'].replace(76,'Ireland',inplace=True)\n",
    "medData = medData[(medData['Breast Cancer?']=='Yes') & (medData['Gender'] == 'Female')]\n",
    "medData = medData.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "predCol = pd.read_excel('Additional BCD Info 3.1.19 De-identified.xlsx',usecols=\"A,DZ\")\n",
    "medData = medData.merge(predCol,left_on='ID #', right_on='ID #')\n",
    "medData = medData[medData[col_to_predict]!='Unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_repititive_blocks = [\n",
    "    (47,89,7),(89,117,7),(117,152,7),\n",
    "    (163,191,7),(197,213,4),(214,249,5),\n",
    "    (294,348,6),(349,356,7),(357,364,7),\n",
    "    (427,435,8)\n",
    "]\n",
    "ind_not_del = list(range(22,43)) + [274]\n",
    "ind_to_del = [2,3,4,(8,11),13,451,(197,213),(349,356),(386,414),(448,469)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processes blocks of columns that contain the same type of information\n",
    "# groups them and returns them in lists.\n",
    "rep_cols = get_repetitive_cols(medData)\n",
    "# remove empty blocks and replace block[1] with an empty list\n",
    "# if there is no cont. element\n",
    "rep_cols_np = [\n",
    "    [\n",
    "        (block[0].astype('int64').values, block[1].astype('float32').values)\n",
    "        if isinstance(block[1],pd.DataFrame) and block[1].shape[1]>0 \n",
    "        else (block[0].astype('int64').values,[]) for block in group  if block\n",
    "    ] \n",
    "    for group in rep_cols\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features for country of origin columns\n",
    "country_of_origin_data = get_country_of_origin_data(medData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixed_cols=[]\n",
    "# for ii, (a,b) in enumerate(medData.dtypes.iteritems()):\n",
    "#     if b == object:\n",
    "#         types = set([type(k) for k in list(medData[a].dropna() ) ])\n",
    "#         if str in types and len(types)>1: \n",
    "#             print(ii,a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns that should only contain real numbered values, but contain strings in the raw data.\n",
    "# this part of the code saves such string values in a json file, where we can specify\n",
    "# the values to replace them with.\n",
    "mixedCols_write_strs = [92,121,126,131,136,141,146,151,152,192,294,303,416,417,418,446,449]\n",
    "dict_replaceColVals = df_replaceColVals_vars(medData,mixedCols_write_strs,str_vals=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "medData.replace(dict_replaceColVals,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/serkan/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/serkan/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/serkan/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:140: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/home/serkan/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:110: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/serkan/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:132: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# replace outlier values with the values given in the json file\n",
    "# where we can specify the values to replace them with\n",
    "dict_replaceColVals_cont = df_replaceColVals_vars(medData,str_vals=False, cont_vals=True)\n",
    "for k,v in dict_replaceColVals_cont.items():\n",
    "    k_l, v_l = [], []\n",
    "    for key, val in v.items():\n",
    "        k_l.append(float(key)); v_l.append(val)\n",
    "    medData[k].replace(k_l,v_l,inplace=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_min_max_count = 250\n",
    "val_count_threshold = 50\n",
    "col_values_replace_OTH = defaultdict(lambda:[])\n",
    "cols_to_del=[]\n",
    "for c in ind_to_del: \n",
    "    if isinstance(c,tuple): cols_to_del += list(medData.columns[c[0]:c[1]])\n",
    "    else: cols_to_del.append(medData.columns[c])\n",
    "\n",
    "for ii, (a,b) in enumerate(medData.dtypes.iteritems()):\n",
    "    # do not delete the column in repetitive blocks\n",
    "    in_rep_block = False\n",
    "    for block in ind_repititive_blocks: \n",
    "        if (ii>=block[0]) and (ii < block[1]): in_rep_block = True\n",
    "            \n",
    "    if (not in_rep_block) and (b == object) and (ii not in ind_not_del):\n",
    "        col_counts = medData[a].value_counts()        \n",
    "        col_max_count = col_counts.iloc[0]\n",
    "        # the most frequent value of a column should occur at least 'col_min_max_count' times\n",
    "        if col_max_count < col_min_max_count: cols_to_del.append(a)\n",
    "        elif medData.shape[0]-col_max_count < col_min_max_count: cols_to_del.append(a)\n",
    "#         # for the columns that we preserve, we replace values if their \n",
    "#         # frequency is not above the given threshold 'val_count_threshold'.\n",
    "#         else:\n",
    "#             vals_to_OTH = list(col_counts[col_counts<val_count_threshold].index)\n",
    "#             if vals_to_OTH: \n",
    "#                 col_values_replace_OTH[a] = vals_to_OTH\n",
    "# print(len(col_values_replace_OTH.keys()))\n",
    "# print(col_values_replace_OTH.keys())\n",
    "# print(len(cols_to_del))\n",
    "# print(cols_to_del)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I deleted the datetime values for now. Will be added in the next iterations.\n",
    "cols_to_del += list(medData.select_dtypes(include=['datetime']))\n",
    "# cols processed by the 'get_repetitive_cols()' function\n",
    "cols_repetitive = sum([list(medData.columns[c[0]:c[1]]) for c in ind_repititive_blocks],[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_obj_cols = [\n",
    "    k for k in medData.select_dtypes(include=['object']) \n",
    "        if (k not in cols_to_del + cols_repetitive +[col_to_predict]) and \n",
    "           (not medData[k].isnull().all() )    \n",
    "]\n",
    "remaining_float_cols = [\n",
    "    k for k in medData.select_dtypes(include=['float64']) \n",
    "        if (k not in cols_to_del + cols_repetitive +[col_to_predict]) and \n",
    "           (not medData[k].isnull().all() )    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "medData_categ = process_remaining_categ_cols(medData[remaining_obj_cols])\n",
    "medData_float = medData[remaining_float_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rep_cols\n",
    "#country_of_origin_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i,k in enumerate(medData.columns):\n",
    "#     if k in remaining_obj_cols: print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model's inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation splits (80% and 20% respectively)\n",
    "n_patients = medData.shape[0]\n",
    "temp_indices = np.arange(n_patients)\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(temp_indices)\n",
    "train_ind, valid_ind = [\n",
    "    temp_indices[ int(n_patients*c[0]):int(n_patients*c[1]) ]\n",
    "    for c in [(0,0.8), (.8,1.)]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single, unique columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# floats\n",
    "float_fields_scaler = StandardScaler()\n",
    "float_fields_scaler.fit(medData_float.iloc[train_ind])\n",
    "\n",
    "train_x_f = np.nan_to_num(\n",
    "    float_fields_scaler.transform(medData_float.iloc[train_ind])\n",
    ").astype('float32')\n",
    "valid_x_f = np.nan_to_num(\n",
    "    float_fields_scaler.transform(medData_float.iloc[valid_ind])\n",
    ").astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical variables\n",
    "train_x_c = medData_categ.iloc[train_ind].values.astype('int64')\n",
    "valid_x_c = medData_categ.iloc[valid_ind].values.astype('int64')\n",
    "categ_vars_max_vals = list(medData_categ.values.max(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blocks of columns that repeat several times (e.g. `1. relative with condition x`, `1. relative age`, `2. relative with condition x`, `2. relative age` etc.). They are processed sepearately in a way that would allow parameter sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_rep_list_np = [] \n",
    "valid_x_rep_list_np = [] \n",
    "std_scalers_f_rep = []\n",
    "categ_data_rep_maxvals = [] # redundent but useful for now\n",
    "rep_blocks_dims_list = [] #list of tuples of (n_rep, c_n_fields, f_n_fields, c_max_vals)\n",
    "\n",
    "x_rep_list = []\n",
    "for group in rep_cols_np:\n",
    "    rep_cols_c, rep_cols_f = list(zip(*group))\n",
    "    rep_cols_c, rep_cols_f = np.array(rep_cols_c), np.array(rep_cols_f)\n",
    "    # categorical values\n",
    "    g_n_rep, g_n_batch, g_n_fields_c = rep_cols_c.shape\n",
    "    rep_cols_c = rep_cols_c.transpose((1,0,2)).reshape(-1,g_n_fields_c)\n",
    "    categ_data_rep_maxvals.append(list(rep_cols_c.max(axis=0)))\n",
    "    rep_cols_c = rep_cols_c.reshape(g_n_batch,g_n_rep,g_n_fields_c)\n",
    "    # cont values\n",
    "    if rep_cols_f.ndim == 3:\n",
    "        g_n_fields_f = rep_cols_f.shape[-1]\n",
    "        rep_blocks_dims_list.append((g_n_rep,g_n_fields_c,g_n_fields_f, categ_data_rep_maxvals[-1]))\n",
    "        rep_cols_f = rep_cols_f.transpose((1,0,2)).reshape(-1,g_n_fields_f)\n",
    "        g_scaler = StandardScaler()\n",
    "        g_scaler.fit(rep_cols_f[train_ind])\n",
    "        std_scalers_f_rep.append(g_scaler)\n",
    "        rep_cols_f = np.nan_to_num(g_scaler.transform(rep_cols_f))\n",
    "        rep_cols_f = rep_cols_f.reshape(g_n_batch,1,g_n_rep,g_n_fields_f)\n",
    "    else: \n",
    "        rep_blocks_dims_list.append((g_n_rep,g_n_fields_c,0, categ_data_rep_maxvals[-1]))\n",
    "        std_scalers_f_rep.append(None)\n",
    "        rep_cols_f = None\n",
    "    \n",
    "    train_x_rep_list_np.append((\n",
    "        rep_cols_c[train_ind],\n",
    "        rep_cols_f[train_ind] if isinstance(rep_cols_f,np.ndarray) else None \n",
    "        ))\n",
    "    valid_x_rep_list_np.append((\n",
    "        rep_cols_c[valid_ind],\n",
    "        rep_cols_f[valid_ind] if isinstance(rep_cols_f,np.ndarray) else None \n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing the country of columns: `Country of Origin for Patient`, `Country of Origin for Mother`, `Country of Origin for Father`\n",
    "\n",
    "We learn one embedding for each country and use the same country embedding for each of these columns. Note that the countries are featurized, and represented by categorical variables (e.g. continent, continent_subregion, development_level, income_group, etc.) as well as some continuous variables (population_density, birth_rate, death_rate, human_development_index, etc.). We also retain the country_name as a categorical features if a given country has more samples than the given threshold in our dataset. Other countries are solely represented by the features that are mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_of_origin_np =  [\n",
    "        (block[0].astype('int64').values, block[1].astype('float32').values)\n",
    "        for block in country_of_origin_data\n",
    "    ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_cols_c, country_cols_f = list(zip(*country_of_origin_np))\n",
    "country_cols_c, country_cols_f = np.array(country_cols_c), np.array(country_cols_f)\n",
    "# categorical values\n",
    "g_n_rep, g_n_batch, g_n_fields_c = country_cols_c.shape\n",
    "country_cols_c = country_cols_c.transpose((1,0,2)).reshape(-1,g_n_fields_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_cols_c, country_cols_f = list(zip(*country_of_origin_np))\n",
    "country_cols_c, country_cols_f = np.array(country_cols_c), np.array(country_cols_f)\n",
    "# categorical values\n",
    "g_n_rep, g_n_batch, g_n_fields_c = country_cols_c.shape\n",
    "country_cols_c = country_cols_c.transpose((1,0,2)).reshape(-1,g_n_fields_c)\n",
    "country_data_maxvals = list(country_cols_c.max(axis=0))\n",
    "country_cols_c = country_cols_c.reshape(g_n_batch,g_n_rep,g_n_fields_c)\n",
    "# cont values\n",
    "g_n_fields_f = country_cols_f.shape[-1]\n",
    "country_block_dims = (g_n_rep,g_n_fields_c,g_n_fields_f, country_data_maxvals)  #\n",
    "country_cols_f = country_cols_f.transpose((1,0,2)).reshape(-1,g_n_fields_f)\n",
    "g_scaler = StandardScaler()\n",
    "g_scaler.fit(country_cols_f[train_ind])\n",
    "std_scalers_f_country = g_scaler    #\n",
    "country_cols_f = np.nan_to_num(g_scaler.transform(country_cols_f))\n",
    "country_cols_f = country_cols_f.reshape(g_n_batch,1,g_n_rep,g_n_fields_f)\n",
    "\n",
    "train_x_country_np = (              #\n",
    "    country_cols_c[train_ind],\n",
    "    country_cols_f[train_ind]\n",
    "    )\n",
    "valid_x_country_np = (              #\n",
    "    country_cols_c[valid_ind],\n",
    "    country_cols_f[valid_ind]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predicted variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = (medData[col_to_predict] == 'Yes').values.astype('int64')\n",
    "y_train, y_valid = y[train_ind], y[valid_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the dataset and prepare the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NYULH_DS(Dataset):\n",
    "    def __init__(self, x_f, x_c, x_rep_list_np, x_country_np, y):\n",
    "        super(NYULH_DS, self).__init__()\n",
    "        self.batch_size = x_f.shape[0]\n",
    "        self.x_rep_list_np = x_rep_list_np\n",
    "        self.x_country_np = x_country_np\n",
    "        assert x_f.shape[0] == x_c.shape[0] == x_country_np[0].shape[0] == x_country_np[1].shape[0], 'number of rows do not match'\n",
    "        for ii, arr in enumerate(x_rep_list_np): \n",
    "            assert x_f.shape[0] == arr[0].shape[0],\\\n",
    "            f'number of rows do not match for the categorical data at index {ii}'\n",
    "            if isinstance(arr[1],np.ndarray) : \n",
    "                assert x_f.shape[0] == arr[1].shape[0], \\\n",
    "                f'number of rows do not match for the cont. data at index {ii}'\n",
    "        \n",
    "        self.x_f = x_f\n",
    "        self.x_c = x_c\n",
    "        self.x_f_rep_list_np = x_rep_list_np\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return self.batch_size\n",
    "        \n",
    "    def __getitem__(self, i):     \n",
    "        return (\n",
    "            (\n",
    "                self.x_f[i], \n",
    "                self.x_c[i], \n",
    "                [(k[0][i],k[1][i] if isinstance(k[1],np.ndarray) else np.array(()))\n",
    "                 for k in self.x_rep_list_np],\n",
    "                (self.x_country_np[0][i],self.x_country_np[1][i])\n",
    "            ),\n",
    "            self.y[i]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/serkan/anaconda3/lib/python3.7/site-packages/torch/utils/data/sampler.py:115: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.weights = torch.tensor(weights, dtype=torch.double)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "n_draws_t = len(train_ind) - (len(train_ind)%batch_size)\n",
    "n_draws_v = len(valid_ind) - (len(train_ind)%batch_size)\n",
    "ds_train = NYULH_DS(train_x_f, train_x_c, train_x_rep_list_np, train_x_country_np, y[train_ind])\n",
    "ds_valid = NYULH_DS(valid_x_f, valid_x_c, valid_x_rep_list_np, valid_x_country_np, y[valid_ind])\n",
    "# balance classes\n",
    "weights_weights_train = 1/torch.tensor([(y[train_ind]==0).sum(), (y[train_ind]==1).sum()], dtype=torch.float)\n",
    "weights_weights_valid = 1/torch.tensor([(y[valid_ind]==0).sum(), (y[valid_ind]==1).sum()], dtype=torch.float)\n",
    "samples_weights_train = weights_weights_train[y[train_ind]]\n",
    "samples_weights_valid = weights_weights_valid[y[valid_ind]]\n",
    "\n",
    "t_sampler = torch.utils.data.sampler.WeightedRandomSampler(samples_weights_train.clone().detach(), n_draws_t)\n",
    "v_sampler = torch.utils.data.sampler.WeightedRandomSampler(samples_weights_valid.clone().detach(), n_draws_v)\n",
    "train_loader = DataLoader(ds_train,batch_size=batch_size, sampler=t_sampler, num_workers=1)\n",
    "valid_loader = DataLoader(ds_valid,batch_size=batch_size, sampler=v_sampler, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_On_Blocks(nn.Module):\n",
    "    '''\n",
    "    This module is used for inputs that consists blocks of repetitive information, for example:\n",
    "    `1. relative with condition x`, `1. relative age`, `2. relative with condition x`, `2. relative age`,..\n",
    "    In such cases, we should be indifferent to where the information is given, and use it the same way\n",
    "    regardless of the block that it is given in. We induce this behavior by using convolution operations\n",
    "    and by doing parameter sharing.\n",
    "    Input of the model is a tuple of (`categorical vars`, `continuous vars`)\n",
    "    '''\n",
    "    def __init__(\n",
    "        self, n_rep, c_n_fields, f_n_fields, c_max_vals, n_hid_func=None, n_emb_func=None, country_data=False\n",
    "    ):\n",
    "        super(Conv_On_Blocks, self).__init__()\n",
    "        assert n_rep>=0 and c_n_fields >= 0 and f_n_fields>=0 and c_n_fields+f_n_fields> 0, 'conv_on_block: invalid input'\n",
    "        self.n_rep = n_rep\n",
    "        self.c_n_fields = c_n_fields\n",
    "        self.f_n_fields = f_n_fields\n",
    "        self.categ_input, self.float_input = c_n_fields>0, f_n_fields>0\n",
    "        \n",
    "        self.n_emb_func = n_emb_func if n_emb_func else lambda x: int(np.sqrt(x))+1 \n",
    "        self.n_hid_func = n_hid_func if n_hid_func else lambda x: int(np.round(x/2+1))\n",
    "        \n",
    "        self.country_data = country_data\n",
    "        \n",
    "        if self.categ_input:\n",
    "            self.embeddings = nn.ModuleList(\n",
    "                [nn.Embedding(k+1,self.n_emb_func(k+1)) for k in c_max_vals]\n",
    "            )\n",
    "            self.c_n_hidden = sum([self.n_emb_func(k+1) for k in c_max_vals])\n",
    "        else:\n",
    "            self.c_n_hidden = 0\n",
    "            \n",
    "        self.n_hidden_1 = max(3, self.n_hid_func(self.f_n_fields+self.c_n_hidden))\n",
    "        self.n_hidden_2 = max(3, self.n_hid_func(self.n_hidden_1))\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=self.n_hidden_1, kernel_size=(1,self.f_n_fields+self.c_n_hidden))\n",
    "        self.conv2 = nn.Conv2d(in_channels=1, out_channels=self.n_hidden_2, kernel_size=(1,self.n_hidden_1))\n",
    "        self.fc1 = nn.Linear(\n",
    "            self.n_hidden_2 if not country_data else self.n_hidden_2*self.n_rep,\n",
    "            self.n_hidden_2 )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.categ_input:\n",
    "            x_c = x[0].view(-1,self.c_n_fields)            \n",
    "            x_c = torch.cat(\n",
    "                [self.embeddings[k](x_c[:,k]) for k in range(self.c_n_fields)],\n",
    "                dim=1\n",
    "            ).reshape(-1, 1, self.n_rep, self.c_n_hidden)\n",
    "            \n",
    "            if self.float_input: x = torch.cat([x_c,x[1]],dim=-1)\n",
    "            else: x = x_c\n",
    "        else:\n",
    "            x = x[1]\n",
    "            \n",
    "        x = F.leaky_relu(self.conv1(x), negative_slope=0.2, inplace=True)\n",
    "        x = x.transpose(1,3)\n",
    "        x = F.leaky_relu(self.conv2(x), 0.2, True).view(-1, self.n_hidden_2, self.n_rep)\n",
    "        if self.country_data: \n",
    "            return self.fc1(x.view(-1,self.n_hidden_2*self.n_rep))\n",
    "        else:\n",
    "            x = F.adaptive_avg_pool2d(x,(self.n_hidden_2,1)) if not self.country_data else x.view(-1,self.n_hidden_2*self.n_rep)\n",
    "            x = self.fc1(x.view(-1, self.n_hidden_2))\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(\n",
    "        self, n_float_fields, n_categ_fields,categ_vars_max_vals, rep_blocks_dims_list, \n",
    "        country_block_dims, latent_size,\n",
    "        x_drop_p = 0.2, h_drop_p=0.5, z_noise=1., train_mode=True, n_emb_func=None\n",
    "    ):\n",
    "        super(Net, self).__init__()\n",
    "        self.n_float_fields = n_float_fields\n",
    "        self.n_categ_fields = n_categ_fields\n",
    "        self.categ_vars_max_vals = categ_vars_max_vals\n",
    "        self.categ_vars_cumsum = [0] + list(np.cumsum(np.array(categ_vars_max_vals)+1))\n",
    "        self.rep_blocks_dims_list = rep_blocks_dims_list\n",
    "        self.country_block_dims = country_block_dims \n",
    "        self.latent_size = latent_size\n",
    "        self.x_drop_p = x_drop_p\n",
    "        self.h_drop_p = h_drop_p\n",
    "        self.z_noise = z_noise\n",
    "        self.train_mode = train_mode\n",
    "        \n",
    "        self.n_emb_func = n_emb_func if n_emb_func else lambda x: int(np.sqrt(x))+1 \n",
    "        \n",
    "        self.x_embeddings = nn.ModuleList(\n",
    "            [nn.Embedding(k+1,self.n_emb_func(k+1)) for k in categ_vars_max_vals]\n",
    "        )\n",
    "        self.c_n_hidden = sum([self.n_emb_func(k+1) for k in categ_vars_max_vals])\n",
    "        \n",
    "        self.x_rep_module = nn.ModuleList(\n",
    "            [Conv_On_Blocks(*k) for k in rep_blocks_dims_list]\n",
    "        )\n",
    "        self.rep_n_hidden = sum([k.fc1.out_features for k in self.x_rep_module])\n",
    "        \n",
    "        self.x_country_module = Conv_On_Blocks(*(list(country_block_dims)+[lambda x: int(np.round(x/4+1)), None, True]) )\n",
    "        self.country_n_hidden = self.x_country_module.fc1.out_features\n",
    "        \n",
    "        self.n_hidden_0 = self.c_n_hidden + self.rep_n_hidden + self.n_float_fields + self.country_n_hidden\n",
    "        self.n_hidden_1 = int(self.n_hidden_0/2 )+1\n",
    "        self.n_hidden_2 = int((self.n_hidden_1+latent_size)/2)\n",
    "        self.n_hidden_y0 = int(latent_size*3/5)\n",
    "        self.n_hidden_d1 = int((n_float_fields+n_categ_fields)*.7)\n",
    "        \n",
    "        self.enc_linear1 = nn.Linear(self.n_hidden_0,self.n_hidden_1)\n",
    "        self.enc_linear2 = nn.Linear(self.n_hidden_1,self.n_hidden_2)\n",
    "        self.enc_linear3 = nn.Linear(self.n_hidden_2,self.latent_size)        \n",
    "        \n",
    "        self.dec_linear1 = nn.Linear(self.latent_size,self.n_hidden_d1)\n",
    "        self.dec_linear2_f = nn.Linear(self.n_hidden_d1,n_float_fields)\n",
    "        self.dec_linear2_c = nn.Linear(self.n_hidden_d1,sum(categ_vars_max_vals) + len(categ_vars_max_vals))\n",
    "        \n",
    "        self.y_linear1 = nn.Linear(self.latent_size,self.n_hidden_y0)\n",
    "        self.y_linear2 = nn.Linear(self.n_hidden_y0,1)\n",
    "                \n",
    "    def forward(self, x, autoencoder=False):\n",
    "        x = torch.cat(\n",
    "            # float vars \n",
    "            [x[0]] +\\\n",
    "            # categ vars\n",
    "            [self.x_embeddings[k](x[1][:,k]) for k in range(self.n_categ_fields)] +\\\n",
    "            # repetitive blocks\n",
    "            [self.x_rep_module[idx](k) for idx,k in enumerate(x[2])] +\\\n",
    "            # country_of_origin features\n",
    "            [self.x_country_module(x[3])],\n",
    "            dim = -1            \n",
    "        )\n",
    "        x = F.leaky_relu(self.enc_linear1(F.dropout(x,self.x_drop_p,self.train_mode,inplace=False)), .2,False)\n",
    "        x = F.leaky_relu(self.enc_linear2(F.dropout(x,self.h_drop_p,self.train_mode,inplace=False)), .2,False)\n",
    "        z = self.enc_linear3(F.dropout(x,self.h_drop_p,self.train_mode,inplace=True))\n",
    "        x = z + torch.randn_like(z, requires_grad=False)*self.z_noise if self.train_mode else z\n",
    "        \n",
    "        if autoencoder:            \n",
    "            x = F.leaky_relu(self.dec_linear1(x), .2,True)\n",
    "            x_f = self.dec_linear2_f(F.dropout(x,self.h_drop_p,self.train_mode,inplace=False))\n",
    "            x_c = self.dec_linear2_c(F.dropout(x,self.h_drop_p,self.train_mode,inplace=False))\n",
    "            x_c = [\n",
    "                x_c[:,self.categ_vars_cumsum[i]:self.categ_vars_cumsum[i+1]]\n",
    "                for i in range(len(self.categ_vars_cumsum)-1)\n",
    "            ]\n",
    "            return x_f,x_c,z\n",
    "        else:\n",
    "            x = F.leaky_relu(self.y_linear1(x), .2,False)\n",
    "            x = self.y_linear2(F.dropout(x,self.h_drop_p,self.train_mode,inplace=True))\n",
    "            return x,z\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x,y in train_loader:\n",
    "#     break\n",
    "# temp_b_conv = Conv_On_Blocks(7,5,3,categ_data_rep_maxvals[0])\n",
    "# temp_b_conv(x[2][0]).shape\n",
    "\n",
    "# tempnet = Net(16,81,categ_vars_max_vals,rep_blocks_dims_list,country_block_dims,20)\n",
    "# tempnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(\n",
    "    n_float_fields=16,\n",
    "    n_categ_fields=81,\n",
    "    categ_vars_max_vals=categ_vars_max_vals,\n",
    "    rep_blocks_dims_list=rep_blocks_dims_list,\n",
    "    country_block_dims=country_block_dims,\n",
    "    latent_size=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii, (data, target) in enumerate(train_loader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, autoencoder=False, coef_z_l2=1, coef_f=1, coef_c=.01, coef_y=1):\n",
    "    model.train(); model.train_mode=True\n",
    "    total_data = 0 \n",
    "    correct_pred = 0\n",
    "    for ii, (data, target) in enumerate(train_loader):\n",
    "        # send to device\n",
    "        for ind,inp in enumerate(data):\n",
    "            if ind < 2: data[ind] = inp.to(device)\n",
    "            elif ind==2: data[ind] = [(k[0].to(device),k[1].to(device)) for k in inp]\n",
    "            else: data[ind] = (inp[0].to(device), inp[1].to(device))\n",
    "        target = target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data, autoencoder)\n",
    "        \n",
    "        if autoencoder:\n",
    "            loss_z_l2 = torch.sum(output[2].pow(2) ) # l2-loss for the latent space\n",
    "            loss_float = F.mse_loss(output[0],data[0])\n",
    "            loss_categ = sum([\n",
    "                F.cross_entropy(inp, targ) for inp,targ in zip(output[1],[data[1][:,k] for k in range(data[1].size(1))])\n",
    "            ])\n",
    "            \n",
    "            loss = coef_z_l2*loss_z_l2 + coef_f*loss_float + coef_c*loss_categ\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # losses - exponential moving averages\n",
    "            new_losses = [loss.item(),loss_z_l2.item(), loss_float.item(), loss_categ.item()]\n",
    "            if ii == 0: losses = new_losses\n",
    "            else: losses = [0.90*losses[ind] + 0.10*l for ind,l in enumerate(new_losses)]\n",
    "        else:\n",
    "            loss_pred = F.binary_cross_entropy_with_logits(output[0].flatten(), target.float())\n",
    "            loss_z_l2 = torch.sum(output[1].pow(2) ) # l2-loss for the latent space\n",
    "            loss = coef_z_l2*loss_z_l2 + coef_y*loss_pred\n",
    "            y_pred = (F.sigmoid(output[0])>=.5)\n",
    "            total_data += target.size(0)\n",
    "            correct_pred += (y_pred.int()==target.int()).cpu().sum().item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # losses - exponential moving averages\n",
    "            new_losses = [loss.item(), loss_pred.item(), loss_z_l2.item()]\n",
    "            if ii == 0: losses = new_losses\n",
    "            else: losses = [0.90*losses[ind] + 0.10*l for ind,l in enumerate(new_losses)]\n",
    "\n",
    "    if autoencoder:\n",
    "        print(\n",
    "            'Train Epoch: {}\\t\\tLoss: {:.6f}\\tz_l2: {:.6f}\\tLoss_float: {:.6f}\\tLoss_categ: {:.6f}'.format(\n",
    "                epoch, losses[0],losses[1],losses[2],losses[3]\n",
    "            ))\n",
    "    else:\n",
    "        print(\n",
    "            'Train Epoch: {}\\t\\tLoss: {:.6f}\\tLoss_pred: {:.6f}\\tz_l2: {:.6f}\\tAccuracy: {:.3}'.format(\n",
    "                        epoch, \n",
    "                        losses[0],losses[1],losses[2], correct_pred/total_data                  \n",
    "                ))\n",
    "        \n",
    "            \n",
    "def test(model, autoencoder=False, coef_z_l2=1, coef_f=1, coef_c=.01, coef_y=1):\n",
    "    model.eval(); model.train_mode=False\n",
    "    total_data = 0 \n",
    "    correct_pred = 0\n",
    "    batch_sizes = []\n",
    "    losses = []\n",
    "    for data, target in valid_loader:\n",
    "        batch_sizes.append(data[0].size(0))\n",
    "        total_data += data[0].size(0)\n",
    "        # send to device\n",
    "        for ind,inp in enumerate(data):\n",
    "            if ind < 2: data[ind] = inp.to(device)\n",
    "            elif ind==2: data[ind] = [(k[0].to(device),k[1].to(device)) for k in inp]\n",
    "            else: data[ind] = (inp[0].to(device), inp[1].to(device))\n",
    "        target = target.to(device)\n",
    "        # permute pixels\n",
    "        output = model(data, autoencoder)\n",
    "                \n",
    "        if autoencoder:\n",
    "            loss_z_l2 = torch.sum(output[2].pow(2) ) # l2-loss for the latent space\n",
    "            loss_float = F.mse_loss(output[0],data[0])\n",
    "            loss_categ = sum([\n",
    "                F.cross_entropy(inp, targ) for inp,targ in zip(output[1],[data[1][:,k] for k in range(data[1].size(1))])\n",
    "            ])\n",
    "            \n",
    "            loss = coef_z_l2*loss_z_l2 + coef_f*loss_float + coef_c*loss_categ\n",
    "            \n",
    "            # exponential moving averages\n",
    "            losses.append([loss.item(),loss_z_l2.item(), loss_float.item(), loss_categ.item()])\n",
    "        else:\n",
    "            loss_pred = F.binary_cross_entropy_with_logits(output[0].flatten(), target.float())\n",
    "            loss_z_l2 = torch.sum(output[1].pow(2) ) # l2-loss for the latent space\n",
    "            loss = coef_z_l2*loss_z_l2 + coef_y*loss_pred\n",
    "            y_pred = (F.sigmoid(output[0])>=.5)\n",
    "            correct_pred += (y_pred.int()==target.int()).cpu().sum().item()\n",
    "            \n",
    "            losses.append([loss.item(), loss_pred.item(), loss_z_l2.item()])\n",
    "        \n",
    "    #corrected by the batch sizes (size of the last batch may be different)\n",
    "    losses_mean = (np.array(losses) * np.array(batch_sizes)[:,None]).sum(axis=0)/total_data\n",
    "    if autoencoder:\n",
    "        print(\n",
    "            'Validation:\\t\\t-Loss:{:.6f}\\tz_l2: {:.6f}\\t-Loss_float:{:.6f}\\t-Loss_categ:{:.6f}'.format(\n",
    "                losses_mean[0],losses_mean[1],losses_mean[2],losses_mean[3]\n",
    "            ))\n",
    "    else:\n",
    "        print(\n",
    "            'Validation \\t\\tLoss: {:.6f}\\tLoss_pred: {:.6f}\\tz_l2: {:.6f}\\tAccuracy: {:.3}'.format(\n",
    "                losses_mean[0],losses_mean[1], losses_mean[2], correct_pred/total_data                  \n",
    "            ))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    " coef_z_schedule = [x for x in [0.,0.,.2,.5,1.] for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0\t\tLoss: 1.571613\tz_l2: 299.343824\tLoss_float: 0.834020\tLoss_categ: 73.759348\n",
      "Validation:\t\t-Loss:1.411871\tz_l2: 198.320068\t-Loss_float:0.709836\t-Loss_categ:70.203499\n",
      "Train Epoch: 1\t\tLoss: 1.350433\tz_l2: 875.474725\tLoss_float: 0.703729\tLoss_categ: 64.670462\n",
      "Validation:\t\t-Loss:1.108007\tz_l2: 696.870586\t-Loss_float:0.507242\t-Loss_categ:60.076484\n",
      "Train Epoch: 2\t\tLoss: 1.266958\tz_l2: 1339.317336\tLoss_float: 0.660243\tLoss_categ: 60.671522\n",
      "Validation:\t\t-Loss:1.051164\tz_l2: 1061.556289\t-Loss_float:0.498957\t-Loss_categ:55.220699\n",
      "Train Epoch: 3\t\tLoss: 1.151315\tz_l2: 1976.061831\tLoss_float: 0.579668\tLoss_categ: 57.164715\n",
      "Validation:\t\t-Loss:1.007691\tz_l2: 1432.029875\t-Loss_float:0.475062\t-Loss_categ:53.262897\n",
      "Train Epoch: 4\t\tLoss: 1.124114\tz_l2: 2679.291613\tLoss_float: 0.566102\tLoss_categ: 55.801232\n",
      "Validation:\t\t-Loss:1.036800\tz_l2: 1858.259362\t-Loss_float:0.513107\t-Loss_categ:52.369292\n",
      "Train Epoch: 5\t\tLoss: 1.115037\tz_l2: 3565.643541\tLoss_float: 0.565649\tLoss_categ: 54.938838\n",
      "Validation:\t\t-Loss:1.004544\tz_l2: 2436.791279\t-Loss_float:0.485390\t-Loss_categ:51.915362\n",
      "Train Epoch: 6\t\tLoss: 1.031252\tz_l2: 3917.614542\tLoss_float: 0.480744\tLoss_categ: 55.050759\n",
      "Validation:\t\t-Loss:0.941686\tz_l2: 2787.054409\t-Loss_float:0.424500\t-Loss_categ:51.718570\n",
      "Train Epoch: 7\t\tLoss: 1.019420\tz_l2: 4671.109285\tLoss_float: 0.480102\tLoss_categ: 53.931811\n",
      "Validation:\t\t-Loss:0.911157\tz_l2: 3357.298462\t-Loss_float:0.393726\t-Loss_categ:51.743081\n",
      "Train Epoch: 8\t\tLoss: 1.014265\tz_l2: 4922.007332\tLoss_float: 0.475538\tLoss_categ: 53.872719\n",
      "Validation:\t\t-Loss:0.909312\tz_l2: 3762.767634\t-Loss_float:0.396452\t-Loss_categ:51.285924\n",
      "Train Epoch: 9\t\tLoss: 1.009673\tz_l2: 5750.929033\tLoss_float: 0.476366\tLoss_categ: 53.330615\n",
      "Validation:\t\t-Loss:0.926624\tz_l2: 4087.472289\t-Loss_float:0.417460\t-Loss_categ:50.916411\n",
      "Train Epoch: 10\t\tLoss: 1.004724\tz_l2: 6536.828207\tLoss_float: 0.469395\tLoss_categ: 53.532892\n",
      "Validation:\t\t-Loss:0.902088\tz_l2: 4202.692581\t-Loss_float:0.393420\t-Loss_categ:50.866787\n",
      "Train Epoch: 11\t\tLoss: 1.031988\tz_l2: 7231.792220\tLoss_float: 0.494375\tLoss_categ: 53.761279\n",
      "Validation:\t\t-Loss:0.901287\tz_l2: 5108.865509\t-Loss_float:0.392710\t-Loss_categ:50.857719\n",
      "Train Epoch: 12\t\tLoss: 0.981646\tz_l2: 7778.415171\tLoss_float: 0.452294\tLoss_categ: 52.935169\n",
      "Validation:\t\t-Loss:0.873241\tz_l2: 5079.371016\t-Loss_float:0.368394\t-Loss_categ:50.484672\n",
      "Train Epoch: 13\t\tLoss: 0.993259\tz_l2: 8080.894860\tLoss_float: 0.462250\tLoss_categ: 53.100844\n",
      "Validation:\t\t-Loss:0.876130\tz_l2: 6318.951259\t-Loss_float:0.372446\t-Loss_categ:50.368431\n",
      "Train Epoch: 14\t\tLoss: 0.983341\tz_l2: 9519.481246\tLoss_float: 0.457766\tLoss_categ: 52.557535\n",
      "Validation:\t\t-Loss:0.883736\tz_l2: 6176.062949\t-Loss_float:0.381032\t-Loss_categ:50.270312\n",
      "Train Epoch: 15\t\tLoss: 0.969190\tz_l2: 9928.523915\tLoss_float: 0.443280\tLoss_categ: 52.590968\n",
      "Validation:\t\t-Loss:0.868084\tz_l2: 6815.283838\t-Loss_float:0.361271\t-Loss_categ:50.681263\n",
      "Train Epoch: 16\t\tLoss: 0.980399\tz_l2: 9911.623622\tLoss_float: 0.462605\tLoss_categ: 51.779370\n",
      "Validation:\t\t-Loss:0.852883\tz_l2: 7099.405400\t-Loss_float:0.344551\t-Loss_categ:50.833264\n",
      "Train Epoch: 17\t\tLoss: 0.984857\tz_l2: 10569.244534\tLoss_float: 0.461880\tLoss_categ: 52.297697\n",
      "Validation:\t\t-Loss:0.880921\tz_l2: 7048.477275\t-Loss_float:0.372916\t-Loss_categ:50.800586\n",
      "Train Epoch: 18\t\tLoss: 0.959274\tz_l2: 10867.422268\tLoss_float: 0.446429\tLoss_categ: 51.284449\n",
      "Validation:\t\t-Loss:0.859939\tz_l2: 7088.100251\t-Loss_float:0.350380\t-Loss_categ:50.955832\n",
      "Train Epoch: 19\t\tLoss: 0.943554\tz_l2: 11870.436239\tLoss_float: 0.419515\tLoss_categ: 52.403874\n",
      "Validation:\t\t-Loss:0.873745\tz_l2: 8048.534516\t-Loss_float:0.368733\t-Loss_categ:50.501198\n",
      "Train Epoch: 20\t\tLoss: 7.839409\tz_l2: 31.815980\tLoss_float: 0.828899\tLoss_categ: 64.731325\n",
      "Validation:\t\t-Loss:2.102656\tz_l2: 3.448655\t-Loss_float:0.785750\t-Loss_categ:62.717480\n",
      "Train Epoch: 21\t\tLoss: 4.163074\tz_l2: 13.293924\tLoss_float: 0.853504\tLoss_categ: 65.078568\n",
      "Validation:\t\t-Loss:1.680545\tz_l2: 1.419533\t-Loss_float:0.779008\t-Loss_categ:61.763004\n",
      "Train Epoch: 22\t\tLoss: 3.266289\tz_l2: 9.199115\tLoss_float: 0.790567\tLoss_categ: 63.589893\n",
      "Validation:\t\t-Loss:1.540627\tz_l2: 0.845031\t-Loss_float:0.753801\t-Loss_categ:61.782001\n",
      "Train Epoch: 23\t\tLoss: 2.814485\tz_l2: 6.980487\tLoss_float: 0.794671\tLoss_categ: 62.371609\n",
      "Validation:\t\t-Loss:1.584045\tz_l2: 0.792147\t-Loss_float:0.800467\t-Loss_categ:62.514910\n",
      "Train Epoch: 24\t\tLoss: 2.574142\tz_l2: 5.670167\tLoss_float: 0.811016\tLoss_categ: 62.909183\n",
      "Validation:\t\t-Loss:1.509202\tz_l2: 0.537618\t-Loss_float:0.781335\t-Loss_categ:62.034312\n",
      "Train Epoch: 25\t\tLoss: 2.432474\tz_l2: 4.774874\tLoss_float: 0.852997\tLoss_categ: 62.450306\n",
      "Validation:\t\t-Loss:1.452260\tz_l2: 0.368566\t-Loss_float:0.762209\t-Loss_categ:61.633848\n",
      "Train Epoch: 26\t\tLoss: 2.162983\tz_l2: 3.880057\tLoss_float: 0.757826\tLoss_categ: 62.914524\n",
      "Validation:\t\t-Loss:1.430299\tz_l2: 0.299456\t-Loss_float:0.754741\t-Loss_categ:61.566695\n",
      "Train Epoch: 27\t\tLoss: 2.100344\tz_l2: 3.402549\tLoss_float: 0.795712\tLoss_categ: 62.412153\n",
      "Validation:\t\t-Loss:1.421815\tz_l2: 0.249212\t-Loss_float:0.750109\t-Loss_categ:62.186423\n",
      "Train Epoch: 28\t\tLoss: 2.062112\tz_l2: 3.044349\tLoss_float: 0.815856\tLoss_categ: 63.738633\n",
      "Validation:\t\t-Loss:1.431634\tz_l2: 0.201178\t-Loss_float:0.781139\t-Loss_categ:61.025969\n",
      "Train Epoch: 29\t\tLoss: 1.953393\tz_l2: 2.595135\tLoss_float: 0.799101\tLoss_categ: 63.526575\n",
      "Validation:\t\t-Loss:1.405715\tz_l2: 0.178708\t-Loss_float:0.760096\t-Loss_categ:60.987748\n",
      "Train Epoch: 30\t\tLoss: 2.508895\tz_l2: 2.162447\tLoss_float: 0.796355\tLoss_categ: 63.131647\n",
      "Validation:\t\t-Loss:1.411578\tz_l2: 0.107066\t-Loss_float:0.750918\t-Loss_categ:60.712640\n",
      "Train Epoch: 31\t\tLoss: 2.231786\tz_l2: 1.648468\tLoss_float: 0.782171\tLoss_categ: 62.538119\n",
      "Validation:\t\t-Loss:1.440283\tz_l2: 0.077524\t-Loss_float:0.778268\t-Loss_categ:62.325276\n",
      "Train Epoch: 32\t\tLoss: 2.140880\tz_l2: 1.413190\tLoss_float: 0.805470\tLoss_categ: 62.881473\n",
      "Validation:\t\t-Loss:1.393101\tz_l2: 0.051238\t-Loss_float:0.748559\t-Loss_categ:61.892237\n",
      "Train Epoch: 33\t\tLoss: 2.027765\tz_l2: 1.204899\tLoss_float: 0.793669\tLoss_categ: 63.164664\n",
      "Validation:\t\t-Loss:1.383402\tz_l2: 0.041043\t-Loss_float:0.753400\t-Loss_categ:60.948092\n",
      "Train Epoch: 34\t\tLoss: 2.037472\tz_l2: 1.099940\tLoss_float: 0.855334\tLoss_categ: 63.216869\n",
      "Validation:\t\t-Loss:1.414197\tz_l2: 0.035964\t-Loss_float:0.775974\t-Loss_categ:62.024111\n",
      "Train Epoch: 35\t\tLoss: 1.971580\tz_l2: 1.009327\tLoss_float: 0.842511\tLoss_categ: 62.440514\n",
      "Validation:\t\t-Loss:1.406431\tz_l2: 0.025073\t-Loss_float:0.787018\t-Loss_categ:60.687619\n",
      "Train Epoch: 36\t\tLoss: 1.894915\tz_l2: 0.869617\tLoss_float: 0.837480\tLoss_categ: 62.262635\n",
      "Validation:\t\t-Loss:1.405251\tz_l2: 0.018563\t-Loss_float:0.787133\t-Loss_categ:60.883612\n",
      "Train Epoch: 37\t\tLoss: 1.761008\tz_l2: 0.711422\tLoss_float: 0.774940\tLoss_categ: 63.035756\n",
      "Validation:\t\t-Loss:1.412123\tz_l2: 0.014972\t-Loss_float:0.787153\t-Loss_categ:61.748411\n",
      "Train Epoch: 38\t\tLoss: 1.800675\tz_l2: 0.702464\tLoss_float: 0.825564\tLoss_categ: 62.387910\n",
      "Validation:\t\t-Loss:1.470150\tz_l2: 0.015101\t-Loss_float:0.841839\t-Loss_categ:62.076012\n",
      "Train Epoch: 39\t\tLoss: 1.766618\tz_l2: 0.581456\tLoss_float: 0.851332\tLoss_categ: 62.455834\n",
      "Validation:\t\t-Loss:1.363552\tz_l2: 0.012232\t-Loss_float:0.746189\t-Loss_categ:61.124755\n",
      "Train Epoch: 40\t\tLoss: 1.970559\tz_l2: 0.522422\tLoss_float: 0.814033\tLoss_categ: 63.410466\n",
      "Validation:\t\t-Loss:1.393506\tz_l2: 0.008761\t-Loss_float:0.771947\t-Loss_categ:61.279821\n",
      "Train Epoch: 41\t\tLoss: 1.924390\tz_l2: 0.466788\tLoss_float: 0.831052\tLoss_categ: 62.654903\n",
      "Validation:\t\t-Loss:1.364339\tz_l2: 0.008143\t-Loss_float:0.752424\t-Loss_categ:60.377179\n",
      "Train Epoch: 42\t\tLoss: 1.782702\tz_l2: 0.362248\tLoss_float: 0.794196\tLoss_categ: 62.625757\n",
      "Validation:\t\t-Loss:1.399334\tz_l2: 0.006289\t-Loss_float:0.770674\t-Loss_categ:62.237048\n",
      "Train Epoch: 43\t\tLoss: 1.881751\tz_l2: 0.325242\tLoss_float: 0.930885\tLoss_categ: 62.562371\n",
      "Validation:\t\t-Loss:1.391799\tz_l2: 0.005712\t-Loss_float:0.778396\t-Loss_categ:60.769100\n",
      "Train Epoch: 44\t\tLoss: 1.740121\tz_l2: 0.298663\tLoss_float: 0.811319\tLoss_categ: 63.013931\n",
      "Validation:\t\t-Loss:1.423742\tz_l2: 0.004879\t-Loss_float:0.808449\t-Loss_categ:61.041427\n",
      "Train Epoch: 45\t\tLoss: 1.730890\tz_l2: 0.251389\tLoss_float: 0.847190\tLoss_categ: 63.231143\n",
      "Validation:\t\t-Loss:1.376234\tz_l2: 0.003573\t-Loss_float:0.759525\t-Loss_categ:61.313625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 46\t\tLoss: 1.680252\tz_l2: 0.217799\tLoss_float: 0.834241\tLoss_categ: 62.821154\n",
      "Validation:\t\t-Loss:1.418896\tz_l2: 0.003187\t-Loss_float:0.807624\t-Loss_categ:60.808510\n",
      "Train Epoch: 47\t\tLoss: 1.633832\tz_l2: 0.183433\tLoss_float: 0.823134\tLoss_categ: 62.726461\n",
      "Validation:\t\t-Loss:1.371069\tz_l2: 0.002369\t-Loss_float:0.754376\t-Loss_categ:61.432328\n",
      "Train Epoch: 48\t\tLoss: 1.649320\tz_l2: 0.190045\tLoss_float: 0.827140\tLoss_categ: 63.213542\n",
      "Validation:\t\t-Loss:1.346384\tz_l2: 0.001982\t-Loss_float:0.738272\t-Loss_categ:60.612965\n",
      "Train Epoch: 49\t\tLoss: 1.641933\tz_l2: 0.179194\tLoss_float: 0.834212\tLoss_categ: 62.852715\n",
      "Validation:\t\t-Loss:1.361706\tz_l2: 0.002085\t-Loss_float:0.740189\t-Loss_categ:61.943215\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "for epoch in range(0,50):\n",
    "    train(epoch, model,coef_z_l2 = coef_z_schedule[epoch], autoencoder=True)\n",
    "    test(model, coef_z_l2 = coef_z_schedule[epoch], autoencoder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "for epoch in range(0,10):\n",
    "    train(epoch, model,coef_z_l2 = 0, autoencoder=False)\n",
    "    test(model, coef_z_l2 = 0, autoencoder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
